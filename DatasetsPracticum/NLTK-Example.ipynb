{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the NLTK functions available.\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features are not installed by default and need to be installed manually\n",
    "by invoking the following commands via the command line:\n",
    "\n",
    "To open a Python session:\n",
    "\n",
    "`> python`\n",
    "  \n",
    "Make nltk available:\n",
    "\n",
    "`> import nltk`\n",
    "  \n",
    "Start the NLTK downloader:\n",
    "\n",
    "`> nltk.download()`\n",
    "  \n",
    "Now a program starts, make sure that under\n",
    "* 'Corpora', alpino - Alpino Dutch Treebank\n",
    "* 'Corpora', treebank - Penn Treebank Sample\n",
    "* 'Models', averaged_perceptron_tagger - Averaged Perceptron Tagger\n",
    "* 'Models', maxent_treebank_pos_tagger - Treebank Part of Speech Tagger (Maximum entropy)\n",
    "* 'Models', tagset - Help on Tagsets\n",
    "\n",
    "is installed, or do so manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to replace the filenames `\"2016.txt\"` with one of your own files of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From http://www.nltk.org/book/ch02.html 1.9 Loading your own Corpus\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "# Use the following root folder that contains the documents of interest.\n",
    "corpusRoot = './per_year'\n",
    "# Put all files in the root folder in a corpus.\n",
    "wordlists = PlaintextCorpusReader(corpusRoot, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all file names in the corpus.\n",
    "print(\"These filenames are in folder {}\".format(corpusRoot))\n",
    "print(wordlists.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all the words in this file.\n",
    "print(\"\\nThese words are in file 2016.txt\")\n",
    "print(wordlists.words('2016.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the words and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the words from this file.\n",
    "words = wordlists.words('2016.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make all words lowercase.\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the stopword list.\n",
    "from nltk.corpus import stopwords\n",
    "stopwordList = []#stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stopwords are luckily provided by NLTK for Dutch.\n",
    "\n",
    "If you want to use some other language not provided by NLTK or a stopword list of your own, remove the `#`'s below and run the code below.\n",
    "Your own stopword list should be a comma-separated list of words, such as:\n",
    "`en,de,het`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopwordList = []\n",
    "#with open(\"stopwords.txt\") as f:\n",
    "#    stopwords = f.read().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out all stopwords.\n",
    "words = [word for word in words if not word in stopwordList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the word frequencies with a frequency distribution.\n",
    "fdist = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the 10 most common words with their frequencies.\n",
    "print(\"\\nThese are the 10 most common words\")\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    print(u'{}\\t{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above creates a frequency distribution object first, and then asks for the most common words with the function `most_common` with the parameter `10`.\n",
    "\n",
    "Note that you can change this parameter to any number of most common words you want.\n",
    "\n",
    "Stopwords will bloat the results of this method. If any such words slipped through, add those words to the stoplist and repeat the stopword removal process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From http://www.nltk.org/book/ch05.html\n",
    "\n",
    "# Use the default Penn Treebank tagset.\n",
    "# A complete overview is available here http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "# Request help on a tag (definition & examples) with:\n",
    "print(\"\\nThe tag 'NN' means:\")\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tag the words with parts of speech as in the Penn Treebank tagset.\n",
    "taggedWords = nltk.pos_tag(words)\n",
    "print(\"\\nThe first 10 words are tagged as:\")\n",
    "print(taggedWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or using the Dutch tagger for more accurate results.\n",
    "# (In fact, don't use the tagger above for non-English texts)\n",
    "# Created using: https://github.com/evanmiltenburg/Dutch-tagger\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.load('model.perc.dutch_tagger_small.pickle')\n",
    "\n",
    "taggedWords = tagger.tag(words)\n",
    "print(\"\\nThe first 10 words are tagged as:\")\n",
    "print(taggedWords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`taggedWords` contains all the words and their tags.\n",
    "You can find interesting facts by using `print(taggedWords)` for all words, or using slicing `print(taggedWords[x:y])` to select all words between `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are multiple words that commonly occur together.\n",
    "Some examples are:\n",
    "- `(hundred, years)`\n",
    "- `(living, creature)`\n",
    "We focus on 2-word combinations (bigrams) for now.\n",
    "Any kind of word combinations can be used: 3-word (trigram), ..., `n`-word (`n`-gram).\n",
    "\n",
    "If you're interested in the latter, use the reference materials below or search for NLTK `n`-gram methods yourself.\n",
    "Adapting the code shouldn't be difficult if you can find NLTK's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From http://www.nltk.org/howto/collocations.html\n",
    "import nltk.collocations\n",
    "import collections\n",
    "\n",
    "bigramMeasures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(taggedWords)\n",
    "# Scored is a list of bigram tuples and their likelihood ratio:\n",
    "#   [((('word1', 'tag1'), ('word2', 'tag2')), likelihood ratio), ...]\n",
    "# For example:\n",
    "#   [((('de', 'IN'), ('president', 'NN'), 0.019015), ...]\n",
    "scored = finder.score_ngrams(bigramMeasures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the first `n`-grams for some tag of interest.\n",
    "By default it is set to singular nouns, but you can change this to any tag that is in the tagger that you used.\n",
    "\n",
    "Note that each element `x` is a `((('word1', 'tag1'), ('word2', 'tag2')), likelihood ratio)`.\n",
    "- So `x[0]` selects `(('word1', 'tag1'), ('word2', 'tag2'))`,\n",
    "- then `x[0][0]` selects `('word1', 'tag1')`,\n",
    "- and finally `x[0][0][1]` selects `'tag1'`.\n",
    "\n",
    "If you're not interested in particular parts of speech, comment out the tag-line by placing a comment sign `#` in front of the line of code.\n",
    "Also remove the comment sign `#` from the line below it to use that line of code.\n",
    "\n",
    "*Commenting/uncommenting lines of code is not good coding practice, but it's easier than a proper workaround for now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter to contain only words tagged as 'nounsg'.\n",
    "scored = [x for x in scored if x[0][0][1] == 'nounsg']\n",
    "#scored = [x for x in scored]\n",
    "\n",
    "# Show the first 5 bigrams.\n",
    "print(\"\\nThe first bigrams found:\")\n",
    "print(scored[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group bigrams by first word in bigram.\n",
    "prefixKeys = collections.defaultdict(list)\n",
    "for key, scores in scored:\n",
    "    prefixKeys[key[0]].append((key[1], scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort keyed bigrams by strongest association.\n",
    "# Highly associated bigrams are placed first.\n",
    "for key in prefixKeys:\n",
    "    prefixKeys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "print(\"\\nThe best bigrams ordered by score:\")\n",
    "print(scored[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Search the collocations for these words where they appear as 'nounsg'.\n",
    "print(\"\\nThe top 5 collocations found:\")\n",
    "print('president:', prefixKeys[('president', 'nounsg')][:5])\n",
    "print('zuid:', prefixKeys[('zuid', 'nounsg')][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the parameters of the collocation search by replacing the search terms and the tag of interest.\n",
    "Use array slicing `[x:y]` to show the results from `x` to `y`.\n",
    "Or `[:y]` for the first `y` results.\n",
    "\n",
    "You can use this to gain understanding of often simultaneously occurring words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is not a native function of NLTK.\n",
    "\n",
    "It is possible to use some other library, such as sci-kit learn.\n",
    "This library is installed with Anaconda.\n",
    "\n",
    "For examples, see\n",
    "http://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php\n",
    "\n",
    "or the `Tf-Idf in Scikit-Learn` part of http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code above is combined to iterate over all the yearly data.\n",
    "You can request data almost similarly to the methods above, but with the addition of selecting a year for that method.\n",
    "\n",
    "Example:\n",
    "`fdist.most_common(10)`\n",
    "becomes\n",
    "`fdists['2016'].most_common(10)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all words.\n",
    "allWords = {}\n",
    "years = [fileid.split('.')[0] for fileid in wordlists.fileids()]\n",
    "for year in years:\n",
    "    originalWords = wordlists.words(year + '.txt')\n",
    "    processedWords = [word.lower() for word in originalWords]\n",
    "    allWords[year] = processedWords\n",
    "\n",
    "# Filter out all stopwords.\n",
    "for year in years:\n",
    "    allWords[year] = [word for word in allWords[year] if not word in stopwordList]  \n",
    "\n",
    "# Create word frequency distributions.\n",
    "fdists = {}\n",
    "for year in years:\n",
    "    fdists[year] = nltk.FreqDist(allWords[year])\n",
    "\n",
    "# Tag all texts.\n",
    "tagged = {}\n",
    "for year in years:\n",
    "    tagged[year] = tagger.tag(allWords[year])\n",
    "\n",
    "\n",
    "# Create bigram scores and prefix keys.\n",
    "bigramScore = {}\n",
    "bigramKeys = {}\n",
    "for year in years:\n",
    "    bigramMeasures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(tagged[year])\n",
    "\n",
    "    scored = finder.score_ngrams(bigramMeasures.likelihood_ratio)\n",
    "\n",
    "    scored = [x for x in scored if x[0][0][1] == 'nounsg']\n",
    "    bigramScore[year] = scored\n",
    "\n",
    "    prefixKeys = collections.defaultdict(list)\n",
    "    for key, scores in scored:\n",
    "        prefixKeys[key[0]].append((key[1], scores))\n",
    "\n",
    "    for key in prefixKeys:\n",
    "        prefixKeys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "    bigramKeys[year] = prefixKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above functions to research some interesting facts.\n",
    "Slight alterations might be necessary, but most of the code below should only be function calls and print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Insert your research code here\n",
    "print(fdists['2016'].most_common(10))\n",
    "\n",
    "print(tagged['2016'][:10])\n",
    "\n",
    "print(bigramScore['2016'][:5])\n",
    "\n",
    "print('president:', bigramKeys['2016'][('president', 'nounsg')][:5])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
